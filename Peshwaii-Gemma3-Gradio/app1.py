import gradio as gr
from unsloth import FastLanguageModel
import torch
from fpdf import FPDF
import csv
import os

hf_token = os.environ.get("HF_TOKEN")

# Load fine-tuned model
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "Devavrat28/peshwai-historian-ai",  # replace with your HF username
    max_seq_length = 2048,
    dtype = torch.float16,
    load_in_4bit = True,
    token=hf_token,  # Add your Hugging Face token here
)

FastLanguageModel.for_inference(model)

# Logging user Q&A
ANSWER_LOG_PATH = "answers.csv"
FEEDBACK_LOG_PATH = "feedback.csv"

def log_query_and_response(question, answer):
    file_exists = os.path.isfile(ANSWER_LOG_PATH)
    with open(ANSWER_LOG_PATH, mode="a", encoding="utf-8", newline="") as file:
        writer = csv.writer(file)
        if not file_exists:
            writer.writerow(["Question", "Answer"])
        writer.writerow([question, answer])

def save_feedback(question, answer, feedback):
    with open(FEEDBACK_LOG_PATH, mode="a", encoding="utf-8", newline="") as file:
        writer = csv.writer(file)
        writer.writerow([question, answer, feedback])

# Generate PDF
def export_answer_as_pdf(answer_text, filename="peshwai_answer.pdf"):
    pdf = FPDF()
    pdf.add_page()
    pdf.set_font("Arial", size=12)
    pdf.multi_cell(0, 10, answer_text)
    pdf.output(filename)
    return filename

# Marathi historian prompt + answer
def generate_marathi_answer(user_input):
    prompt = f"""‡§§‡•Å‡§Æ‡•ç‡§π‡•Ä ‡§è‡§ï ‡§á‡§§‡§ø‡§π‡§æ‡§∏‡§ï‡§æ‡§∞ ‡§Ü‡§π‡§æ‡§§ ‡§Ü‡§£‡§ø ‡§§‡•Å‡§Æ‡§ö‡•á ‡§∏‡§Ç‡§∂‡•ã‡§ß‡§® ‡§™‡•á‡§∂‡§µ‡§æ‡§à ‡§ï‡§æ‡§≤‡§ñ‡§Ç‡§°‡§æ‡§µ‡§∞ ‡§Ü‡§π‡•á. 
‡§ñ‡§æ‡§≤‡•Ä ‡§¶‡§ø‡§≤‡•á‡§≤‡•ç‡§Ø‡§æ ‡§â‡§¶‡§æ‡§π‡§∞‡§£‡§æ‡§Ç‡§™‡•ç‡§∞‡§Æ‡§æ‡§£‡•á ‡§â‡§§‡•ç‡§§‡§∞ ‡§∏‡§µ‡§ø‡§∏‡•ç‡§§‡§∞ ‡§Ü‡§£‡§ø ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä‡§™‡•Ç‡§∞‡•ç‡§£ ‡§¶‡•ç‡§Ø‡§æ:

‡§â‡§¶‡§æ‡§π‡§∞‡§£ ‡•ß:
‡§µ‡§ø‡§∑‡§Ø: ‡§®‡§æ‡§®‡§æ ‡§´‡§°‡§£‡§µ‡•Ä‡§∏‡§æ‡§Ç‡§ö‡•á ‡§ó‡•Å‡§™‡•ç‡§§ ‡§∞‡§æ‡§ú‡§ï‡§æ‡§∞‡§£
‡§∏‡§µ‡§ø‡§∏‡•ç‡§§‡§∞ ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä: ‡§®‡§æ‡§®‡§æ ‡§´‡§°‡§£‡§µ‡•Ä‡§∏ ‡§π‡•á ‡§ï‡•á‡§µ‡§≥ ‡§™‡•á‡§∂‡§µ‡•ç‡§Ø‡§æ‡§Ç‡§ö‡•á ‡§µ‡§ø‡§∂‡•ç‡§µ‡§æ‡§∏‡•Ç ‡§®‡§∏‡•Ç‡§® ‡§§‡•ç‡§Ø‡§æ‡§Ç‡§®‡•Ä '‡§¨‡§æ‡§∞‡§≠‡§æ‡§à ‡§Æ‡§Ç‡§°‡§≥‡§æ'‡§ö‡•ç‡§Ø‡§æ ‡§Æ‡§æ‡§ß‡•ç‡§Ø‡§Æ‡§æ‡§§‡•Ç‡§® ‡§™‡•á‡§∂‡§µ‡•ç‡§Ø‡§æ‡§Ç‡§ö‡•Ä ‡§∏‡§§‡•ç‡§§‡§æ ‡§Ö‡§¨‡§æ‡§ß‡§ø‡§§ ‡§†‡•á‡§µ‡§£‡•ç‡§Ø‡§æ‡§ö‡§æ ‡§™‡•ç‡§∞‡§Ø‡§§‡•ç‡§® ‡§ï‡•á‡§≤‡§æ ‡§π‡•ã‡§§‡§æ. ...

‡§â‡§¶‡§æ‡§π‡§∞‡§£ ‡•®:
‡§µ‡§ø‡§∑‡§Ø: ‡§Æ‡§æ‡§ß‡§µ‡§∞‡§æ‡§µ ‡§™‡•á‡§∂‡§µ‡•ç‡§Ø‡§æ‡§Ç‡§ö‡§æ ‡§Ü‡§∞‡•ã‡§ó‡•ç‡§Ø‡§æ‡§µ‡§∞ ‡§ù‡§æ‡§≤‡•á‡§≤‡§æ ‡§™‡§∞‡§ø‡§£‡§æ‡§Æ
‡§∏‡§µ‡§ø‡§∏‡•ç‡§§‡§∞ ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä: ‡§Æ‡§æ‡§ß‡§µ‡§∞‡§æ‡§µ ‡§™‡•á‡§∂‡§µ‡•á ‡§π‡•á ‡§Ö‡§§‡•ç‡§Ø‡§Ç‡§§ ‡§¨‡•Å‡§¶‡•ç‡§ß‡§ø‡§Æ‡§æ‡§® ‡§π‡•ã‡§§‡•á. ‡§Æ‡§æ‡§§‡•ç‡§∞ ‡§§‡•ç‡§Ø‡§æ‡§Ç‡§ö‡•ç‡§Ø‡§æ ‡§Ö‡§≤‡•ç‡§™ ‡§µ‡§Ø‡§æ‡§§ ‡§Æ‡•É‡§§‡•ç‡§Ø‡•Ç‡§ö‡•á ‡§ï‡§æ‡§∞‡§£ ‡§∞‡§æ‡§ú‡§ï‡•Ä‡§Ø ‡§§‡§£‡§æ‡§µ, ‡§ò‡§∞‡§ó‡•Å‡§§‡•Ä ‡§∏‡§Ç‡§ò‡§∞‡•ç‡§∑ ‡§Ü‡§£‡§ø ‡§∏‡§æ‡§§‡§§‡•ç‡§Ø‡§æ‡§®‡•á ‡§ù‡§æ‡§≤‡•á‡§≤‡•ç‡§Ø‡§æ ‡§≤‡§¢‡§æ‡§Ø‡§æ‡§Ç‡§Æ‡•Å‡§≥‡•á ‡§®‡§ø‡§∞‡•ç‡§Æ‡§æ‡§£ ‡§ù‡§æ‡§≤‡•á‡§≤‡•á ‡§Ü‡§∞‡•ã‡§ó‡•ç‡§Ø‡§æ‡§ö‡•á ‡§¨‡§ø‡§ò‡§æ‡§° ‡§π‡•á ‡§π‡•ã‡§§‡•á...

---

‡§Ü‡§§‡§æ ‡§ñ‡§æ‡§≤‡•Ä‡§≤ ‡§µ‡§ø‡§∑‡§Ø‡§æ‡§µ‡§∞ ‡§â‡§§‡•ç‡§§‡§∞ ‡§≤‡§ø‡§π‡§æ:

‡§µ‡§ø‡§∑‡§Ø: {user_input}
‡§∏‡§µ‡§ø‡§∏‡•ç‡§§‡§∞ ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä:"""

    inputs = tokenizer([prompt], return_tensors="pt", truncation=True, padding=True).to(model.device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=512,
        temperature=0.7,
        top_p=0.85,
        repetition_penalty=1.2,
        no_repeat_ngram_size=3,
        eos_token_id=tokenizer.eos_token_id,
    )
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    final_answer = generated_text.split("‡§∏‡§µ‡§ø‡§∏‡•ç‡§§‡§∞ ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä:")[-1].strip()
    log_query_and_response(user_input, final_answer)
    return final_answer

# Gradio App
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("# üìú ‡§™‡•á‡§∂‡§µ‡§æ‡§à ‡§á‡§§‡§ø‡§π‡§æ‡§∏ - AI ‡§á‡§§‡§ø‡§π‡§æ‡§∏‡§ï‡§æ‡§∞")
    gr.Markdown("‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§ ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§µ‡§ø‡§ö‡§æ‡§∞‡§æ ‡§Ü‡§£‡§ø ‡§™‡•á‡§∂‡§µ‡§æ‡§à ‡§ï‡§æ‡§≥‡§æ‡§§‡•Ä‡§≤ ‡§∏‡§ñ‡•ã‡§≤, ‡§Ö‡§≠‡•ç‡§Ø‡§æ‡§∏‡§™‡•Ç‡§∞‡•ç‡§£ ‡§â‡§§‡•ç‡§§‡§∞ ‡§Æ‡§ø‡§≥‡§µ‡§æ!")

    input_box = gr.Textbox(lines=2, placeholder="‡§â‡§¶‡§æ: ‡§∂‡§®‡§ø‡§µ‡§æ‡§∞ ‡§µ‡§æ‡§°‡•ç‡§Ø‡§æ‡§ö‡•á ‡§ê‡§§‡§ø‡§π‡§æ‡§∏‡§ø‡§ï ‡§Æ‡§π‡§§‡•ç‡§§‡•ç‡§µ ‡§ï‡§æ‡§Ø ‡§Ü‡§π‡•á?", label="‡§§‡•Å‡§Æ‡§ö‡§æ ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§Ø‡•á‡§•‡•á ‡§≤‡§ø‡§π‡§æ:")
    output_box = gr.Textbox(lines=10, label="‡§á‡§§‡§ø‡§π‡§æ‡§∏‡§ï‡§æ‡§∞‡§æ‡§ö‡•á ‡§â‡§§‡•ç‡§§‡§∞")
    feedback_radio = gr.Radio(["‡§π‡•ã‡§Ø", "‡§®‡§æ‡§π‡•Ä"], label="‡§π‡•á ‡§â‡§§‡•ç‡§§‡§∞ ‡§â‡§™‡§Ø‡•Å‡§ï‡•ç‡§§ ‡§π‡•ã‡§§‡•á ‡§ï‡§æ?")
    file_output = gr.File(label="PDF ‡§°‡§æ‡§â‡§®‡§≤‡•ã‡§°")

    generate_btn = gr.Button("‡§â‡§§‡•ç‡§§‡§∞ ‡§Æ‡§ø‡§≥‡§µ‡§æ üöÄ")
    download_btn = gr.Button("‡§â‡§§‡•ç‡§§‡§∞ PDF ‡§Æ‡•ç‡§π‡§£‡•Ç‡§® ‡§°‡§æ‡§â‡§®‡§≤‡•ã‡§° ‡§ï‡§∞‡§æ")

    def handle_all(user_input):
        answer = generate_marathi_answer(user_input)
        return answer

    def generate_pdf(user_input):
        answer = generate_marathi_answer(user_input)
        filename = export_answer_as_pdf(answer)
        return answer, filename

    generate_btn.click(fn=handle_all, inputs=input_box, outputs=output_box)
    download_btn.click(fn=generate_pdf, inputs=input_box, outputs=[output_box, file_output])
    feedback_radio.change(fn=save_feedback, inputs=[input_box, output_box, feedback_radio], outputs=[])

demo.launch(share=True, debug=True)